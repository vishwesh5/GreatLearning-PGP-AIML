{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Babysitting.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "m7avRHNZ1JET",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vDlhx3aOCaZn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### SVHN neural network using Keras\n",
        "\n",
        "### Loading the dataset : Lets us load the training and the test data and check the size of the tensors. Lets us also display the first few images from the training set. (5 points)"
      ]
    },
    {
      "metadata": {
        "id": "cT9-WUyj1KK5",
        "colab_type": "code",
        "outputId": "792467e2-587e-45e0-9f0b-ee62f11b9bfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Open the file as readonly\n",
        "\n",
        "h5f = h5py.File('/content/drive/My Drive/DLCP/Project-1/Data/SVHN_single_grey1.h5', 'r')\n",
        "\n",
        "\n",
        "# Load the training, test and validation set\n",
        "x_train = h5f['X_train'][:]\n",
        "y_train = h5f['y_train'][:]\n",
        "x_val = h5f['X_test'][:]\n",
        "y_val = h5f['y_test'][:]\n",
        "\n",
        "\n",
        "# Close this file\n",
        "h5f.close()\n",
        "\n",
        "print('Training set', x_train.shape, y_train.shape)\n",
        "print('Test set', x_val.shape, y_val.shape)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Flatten the images for keras model\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], 1024)\n",
        "x_val = x_val.reshape(x_val.shape[0], 1024)\n",
        "\n",
        "# # normalize inputs from 0-255 to 0-1\n",
        "x_train = x_train / 255.0\n",
        "x_val = x_val / 255.0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set (42000, 32, 32) (42000,)\n",
            "Test set (18000, 32, 32) (18000,)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EZ5ezJ2NLFbP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3bcb21d6-3623-4a0e-8e43-8f3a8202ffe2"
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "gTaaM2az2jgw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "y_train = to_categorical(y_train, num_classes=num_classes)\n",
        "y_val = to_categorical(y_val, num_classes=num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oxTufIo_2xgW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "748b4487-7521-446b-faf2-259b9e970ab0"
      },
      "cell_type": "code",
      "source": [
        "print(\"('Training set', {}, {})('Test set', {}, {})\".format(x_train.shape, y_train.shape, x_val.shape, y_val.shape))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Training set', (42000, 1024), (42000, 10))('Test set', (18000, 1024), (18000, 10))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0AGnOXUyMTxO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "5288a0d9-ff14-4c36-96db-72d6170b91e7"
      },
      "cell_type": "code",
      "source": [
        "# visualizing the first 10 images in the dataset and their labels\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 1))\n",
        "for i in range(10):\n",
        "    plt.subplot(1, 10, i+1)\n",
        "    plt.imshow(x_train[i].reshape(32, 32), cmap=\"gray\")\n",
        "    plt.title(\"{}\".format(list(y_train[i]).index(max(y_train[i]))))\n",
        "    plt.axis('off')\n",
        "plt.show()\n",
        "# print('label for each of the above image: %s' % (y_train[0:10]))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAABeCAYAAADsdyocAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 720x72 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "6hz4OntICsJ_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Build the Neural Network Model in Keras (10 Points)"
      ]
    },
    {
      "metadata": {
        "id": "rZVmyyRDMf43",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import all the that you need\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.optimizers import SGD\n",
        "from keras import regularizers\n",
        "\n",
        "#define the train and test loops and return score\n",
        "def train_and_test_loop(iterations, lr, Lambda, verb=True):\n",
        "    ## Define hyperparameters\n",
        "    iterations = iterations\n",
        "    learning_rate = lr\n",
        "    hidden_nodes = 32\n",
        "    output_nodes = 10\n",
        "   \n",
        "    # Build the building blocks : Hidden layers, output layers, activation functions, optimizers\n",
        "    model = Sequential()\n",
        "    model.add(Dense(32, input_shape=(1024, ), \n",
        "              kernel_regularizer=regularizers.l2(Lambda), \n",
        "              activity_regularizer=regularizers.l1(Lambda)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(10, input_shape=(1024, ), \n",
        "              kernel_regularizer=regularizers.l2(Lambda), \n",
        "              activity_regularizer=regularizers.l1(Lambda)))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    sgd = SGD(lr=learning_rate, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    # Fit the model\n",
        "    model.fit(x_train, y_train, epochs=iterations, batch_size=32)\n",
        "    \n",
        "    \n",
        "def train_and_test_loop1(iterations, lr, Lambda, verb=True):\n",
        "    ## hyperparameters\n",
        "    iterations = iterations\n",
        "    learning_rate = lr\n",
        "    hidden_nodes = 32\n",
        "    output_nodes = 10\n",
        "\n",
        "    #Build the building blocks : Hidden layers, output layers, activation functions, optimizers\n",
        "    model = Sequential()\n",
        "    model.add(Dense(32, input_shape=(1024, ), \n",
        "              kernel_regularizer=regularizers.l2(Lambda), \n",
        "              activity_regularizer=regularizers.l1(Lambda)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(10, input_shape=(1024, ), \n",
        "              kernel_regularizer=regularizers.l2(Lambda), \n",
        "              activity_regularizer=regularizers.l1(Lambda)))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    sgd = SGD(lr=learning_rate)\n",
        "    \n",
        "    # Compile the model\n",
        "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    # Fit the model\n",
        "    model.fit(x_train, y_train, epochs=iterations, batch_size=32)\n",
        "    \n",
        "    # Calculate score and return score\n",
        "    score = model.evaluate(x_val, y_val, batch_size=16)\n",
        "    return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Dv-3QYLC5y1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Babysitting the learning process. Complete all the steps below to optimize your model (15 points)\n",
        "\n",
        "### Step 1: Double Check that the loss is reasonable : Disable the regularization"
      ]
    },
    {
      "metadata": {
        "id": "Cxt4s0BlvfZF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4e5a2bf1-202c-491f-b779-114efa9870a3"
      },
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "Lambda = 0\n",
        "score = train_and_test_loop(1, lr, Lambda)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "42000/42000 [==============================] - 2s 53us/step - loss: 2.2898 - acc: 0.1315\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5ReSEPL_DCKT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 2: Now, lets crank up the Lambda(Regularization)and check what it does to our loss function."
      ]
    },
    {
      "metadata": {
        "id": "knx4KCTS4UM1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cd39bc48-9f39-4b2b-d1aa-9464af31c9b6"
      },
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "Lambda = 0.01\n",
        "score = train_and_test_loop(1, lr, Lambda)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "42000/42000 [==============================] - 2s 58us/step - loss: 3.5689 - acc: 0.1026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LHYoGZGmDF5b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 3: Now, lets overfit to a small subset of our dataset, in this case 20 images."
      ]
    },
    {
      "metadata": {
        "id": "Q61TGHXWeULH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train_subset = x_train[0:20]\n",
        "y_train_subset = y_train[0:20]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f9R2eu0Bv9Fp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = x_train_subset\n",
        "y_train = y_train_subset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mc0OlNy4v_nQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0efaa8fd-fe90-4916-9fa6-3c493df67b17"
      },
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 1024)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "hoKGf3EYwATC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b68cc3bd-4da6-490a-c5f5-f5567b5153e1"
      },
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "a4Kgf2D5DMw_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tip: Make sure that you can overfit very small portion of the training data\n",
        "So, set a small learning rate and turn regularization off\n",
        "\n",
        "In the code below:\n",
        "- Take the first 20 examples from SVHN\n",
        "- turn off regularization(reg=0.0)\n",
        "- use simple vanilla 'sgd'"
      ]
    },
    {
      "metadata": {
        "id": "PIC9ygUywBOF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2150
        },
        "outputId": "cc22c12b-6f5b-4a7d-9a11-f83b6e2137fa"
      },
      "cell_type": "code",
      "source": [
        "lr = 0.01\n",
        "Lambda = 0.0\n",
        "train_and_test_loop(60, lr, Lambda)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 2.2753 - acc: 0.1000\n",
            "Epoch 2/60\n",
            "20/20 [==============================] - 0s 245us/step - loss: 2.0999 - acc: 0.3000\n",
            "Epoch 3/60\n",
            "20/20 [==============================] - 0s 220us/step - loss: 1.9871 - acc: 0.2500\n",
            "Epoch 4/60\n",
            "20/20 [==============================] - 0s 230us/step - loss: 1.9048 - acc: 0.2500\n",
            "Epoch 5/60\n",
            "20/20 [==============================] - 0s 104us/step - loss: 1.8382 - acc: 0.3000\n",
            "Epoch 6/60\n",
            "20/20 [==============================] - 0s 114us/step - loss: 1.7787 - acc: 0.3000\n",
            "Epoch 7/60\n",
            "20/20 [==============================] - 0s 104us/step - loss: 1.7270 - acc: 0.4000\n",
            "Epoch 8/60\n",
            "20/20 [==============================] - 0s 152us/step - loss: 1.6716 - acc: 0.4500\n",
            "Epoch 9/60\n",
            "20/20 [==============================] - 0s 126us/step - loss: 1.6156 - acc: 0.5000\n",
            "Epoch 10/60\n",
            "20/20 [==============================] - 0s 131us/step - loss: 1.5697 - acc: 0.5500\n",
            "Epoch 11/60\n",
            "20/20 [==============================] - 0s 197us/step - loss: 1.5247 - acc: 0.5000\n",
            "Epoch 12/60\n",
            "20/20 [==============================] - 0s 145us/step - loss: 1.4756 - acc: 0.5500\n",
            "Epoch 13/60\n",
            "20/20 [==============================] - 0s 114us/step - loss: 1.4309 - acc: 0.5000\n",
            "Epoch 14/60\n",
            "20/20 [==============================] - 0s 88us/step - loss: 1.3878 - acc: 0.6000\n",
            "Epoch 15/60\n",
            "20/20 [==============================] - 0s 92us/step - loss: 1.3397 - acc: 0.6500\n",
            "Epoch 16/60\n",
            "20/20 [==============================] - 0s 85us/step - loss: 1.2932 - acc: 0.6500\n",
            "Epoch 17/60\n",
            "20/20 [==============================] - 0s 116us/step - loss: 1.2473 - acc: 0.6500\n",
            "Epoch 18/60\n",
            "20/20 [==============================] - 0s 101us/step - loss: 1.2028 - acc: 0.7500\n",
            "Epoch 19/60\n",
            "20/20 [==============================] - 0s 115us/step - loss: 1.1613 - acc: 0.7500\n",
            "Epoch 20/60\n",
            "20/20 [==============================] - 0s 125us/step - loss: 1.1193 - acc: 0.7000\n",
            "Epoch 21/60\n",
            "20/20 [==============================] - 0s 118us/step - loss: 1.0779 - acc: 0.7500\n",
            "Epoch 22/60\n",
            "20/20 [==============================] - 0s 132us/step - loss: 1.0374 - acc: 0.7500\n",
            "Epoch 23/60\n",
            "20/20 [==============================] - 0s 122us/step - loss: 0.9995 - acc: 0.7500\n",
            "Epoch 24/60\n",
            "20/20 [==============================] - 0s 107us/step - loss: 0.9609 - acc: 0.7500\n",
            "Epoch 25/60\n",
            "20/20 [==============================] - 0s 104us/step - loss: 0.9262 - acc: 0.8000\n",
            "Epoch 26/60\n",
            "20/20 [==============================] - 0s 127us/step - loss: 0.8928 - acc: 0.8500\n",
            "Epoch 27/60\n",
            "20/20 [==============================] - 0s 104us/step - loss: 0.8589 - acc: 0.9000\n",
            "Epoch 28/60\n",
            "20/20 [==============================] - 0s 127us/step - loss: 0.8266 - acc: 0.9000\n",
            "Epoch 29/60\n",
            "20/20 [==============================] - 0s 137us/step - loss: 0.7942 - acc: 0.9000\n",
            "Epoch 30/60\n",
            "20/20 [==============================] - 0s 108us/step - loss: 0.7639 - acc: 0.9000\n",
            "Epoch 31/60\n",
            "20/20 [==============================] - 0s 125us/step - loss: 0.7339 - acc: 0.9000\n",
            "Epoch 32/60\n",
            "20/20 [==============================] - 0s 116us/step - loss: 0.7065 - acc: 0.9000\n",
            "Epoch 33/60\n",
            "20/20 [==============================] - 0s 115us/step - loss: 0.6791 - acc: 0.9000\n",
            "Epoch 34/60\n",
            "20/20 [==============================] - 0s 119us/step - loss: 0.6532 - acc: 0.9500\n",
            "Epoch 35/60\n",
            "20/20 [==============================] - 0s 100us/step - loss: 0.6275 - acc: 0.9500\n",
            "Epoch 36/60\n",
            "20/20 [==============================] - 0s 102us/step - loss: 0.6041 - acc: 0.9500\n",
            "Epoch 37/60\n",
            "20/20 [==============================] - 0s 101us/step - loss: 0.5798 - acc: 1.0000\n",
            "Epoch 38/60\n",
            "20/20 [==============================] - 0s 120us/step - loss: 0.5571 - acc: 1.0000\n",
            "Epoch 39/60\n",
            "20/20 [==============================] - 0s 118us/step - loss: 0.5355 - acc: 1.0000\n",
            "Epoch 40/60\n",
            "20/20 [==============================] - 0s 100us/step - loss: 0.5142 - acc: 1.0000\n",
            "Epoch 41/60\n",
            "20/20 [==============================] - 0s 411us/step - loss: 0.4940 - acc: 1.0000\n",
            "Epoch 42/60\n",
            "20/20 [==============================] - 0s 179us/step - loss: 0.4745 - acc: 1.0000\n",
            "Epoch 43/60\n",
            "20/20 [==============================] - 0s 116us/step - loss: 0.4556 - acc: 1.0000\n",
            "Epoch 44/60\n",
            "20/20 [==============================] - 0s 122us/step - loss: 0.4373 - acc: 1.0000\n",
            "Epoch 45/60\n",
            "20/20 [==============================] - 0s 143us/step - loss: 0.4196 - acc: 1.0000\n",
            "Epoch 46/60\n",
            "20/20 [==============================] - 0s 147us/step - loss: 0.4038 - acc: 1.0000\n",
            "Epoch 47/60\n",
            "20/20 [==============================] - 0s 117us/step - loss: 0.3875 - acc: 1.0000\n",
            "Epoch 48/60\n",
            "20/20 [==============================] - 0s 107us/step - loss: 0.3719 - acc: 1.0000\n",
            "Epoch 49/60\n",
            "20/20 [==============================] - 0s 112us/step - loss: 0.3576 - acc: 1.0000\n",
            "Epoch 50/60\n",
            "20/20 [==============================] - 0s 107us/step - loss: 0.3435 - acc: 1.0000\n",
            "Epoch 51/60\n",
            "20/20 [==============================] - 0s 145us/step - loss: 0.3298 - acc: 1.0000\n",
            "Epoch 52/60\n",
            "20/20 [==============================] - 0s 143us/step - loss: 0.3173 - acc: 1.0000\n",
            "Epoch 53/60\n",
            "20/20 [==============================] - 0s 120us/step - loss: 0.3051 - acc: 1.0000\n",
            "Epoch 54/60\n",
            "20/20 [==============================] - 0s 102us/step - loss: 0.2932 - acc: 1.0000\n",
            "Epoch 55/60\n",
            "20/20 [==============================] - 0s 107us/step - loss: 0.2822 - acc: 1.0000\n",
            "Epoch 56/60\n",
            "20/20 [==============================] - 0s 99us/step - loss: 0.2715 - acc: 1.0000\n",
            "Epoch 57/60\n",
            "20/20 [==============================] - 0s 115us/step - loss: 0.2612 - acc: 1.0000\n",
            "Epoch 58/60\n",
            "20/20 [==============================] - 0s 134us/step - loss: 0.2518 - acc: 1.0000\n",
            "Epoch 59/60\n",
            "20/20 [==============================] - 0s 102us/step - loss: 0.2423 - acc: 1.0000\n",
            "Epoch 60/60\n",
            "20/20 [==============================] - 0s 106us/step - loss: 0.2335 - acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FsCjEokbDQMH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Loading the original dataset again"
      ]
    },
    {
      "metadata": {
        "id": "7uOtj9_EwOAt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "6bcabe6b-9593-4eb7-a3d3-0b1b43bfc83f"
      },
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Open the file as readonly\n",
        "\n",
        "h5f = h5py.File('/content/drive/My Drive/DLCP/Project-1/Data/SVHN_single_grey1.h5', 'r')\n",
        "\n",
        "\n",
        "# Load the training, test and validation set\n",
        "x_train = h5f['X_train'][:]\n",
        "y_train = h5f['y_train'][:]\n",
        "x_val = h5f['X_test'][:]\n",
        "y_val = h5f['y_test'][:]\n",
        "\n",
        "\n",
        "# Close this file\n",
        "h5f.close()\n",
        "\n",
        "print('Training set', x_train.shape, y_train.shape)\n",
        "print('Test set', x_val.shape, y_val.shape)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Flatten the images for keras model\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], 1024)\n",
        "x_val = x_val.reshape(x_val.shape[0], 1024)\n",
        "\n",
        "# # normalize inputs from 0-255 to 0-1\n",
        "x_train = x_train / 255.0\n",
        "x_val = x_val / 255.0\n",
        "\n",
        "num_classes = 10\n",
        "y_train = to_categorical(y_train, num_classes=num_classes)\n",
        "y_val = to_categorical(y_val, num_classes=num_classes)\n",
        "\n",
        "print(\"('Training set', {}, {})('Test set', {}, {})\".format(x_train.shape, y_train.shape, x_val.shape, y_val.shape))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set (42000, 32, 32) (42000,)\n",
            "Test set (18000, 32, 32) (18000,)\n",
            "\n",
            "\n",
            "('Training set', (42000, 1024), (42000, 10))('Test set', (18000, 1024), (18000, 10))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p0ScWlVBDVMD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Step 4: Start with small regularization and find learning rate that makes the loss go down.\n",
        "\n",
        "- we start with Lambda(small regularization) = 1e-7\n",
        "- we start with a small learning rate =1e-7"
      ]
    },
    {
      "metadata": {
        "id": "H-6toRKs1esg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "528bfc74-528c-4606-f3d3-80cce00f90b6"
      },
      "cell_type": "code",
      "source": [
        "#set the hyperparameters according to the above instructions\n",
        "lr = 1e-3 # 1e-7 2.3x to 2.28 for 1e-3, for 1e-2 not less than 1e-3\n",
        "Lambda = 1e-7\n",
        "#call the train and test function\n",
        "train_and_test_loop(1, lr, Lambda)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "42000/42000 [==============================] - 5s 130us/step - loss: 2.2821 - acc: 0.1442\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VVnMDmhuDYMl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 5: Okay now lets try a (larger) learning rate . What could possibly go wrong?\n",
        "\n",
        "- Learning rate lr  \n",
        "- Regularization lambda \n"
      ]
    },
    {
      "metadata": {
        "id": "alrW8Cy81ol7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "cb31f29a-ed62-41a4-e4d2-7a48a6b4b9f6"
      },
      "cell_type": "code",
      "source": [
        "#Set Hyperparameters( High value for lr and low values for lambda)\n",
        "lr_list = [1e-1, 1e-2, 1e-3, 1e-4]\n",
        "lambda_list = [1e-8, 1e-7, 1e-6, 1e-5]\n",
        "for lr in lr_list:\n",
        "  Lambda = lambda_list[0]\n",
        "  print(\"Lambda: {:.8f}, LR: {:.4f}\".format(Lambda, lr), end='\\t')\n",
        "  train_and_test_loop(1, lr, Lambda)\n",
        "# Call the train and test function"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lambda: 0.00000001, LR: 0.1000\tEpoch 1/1\n",
            "42000/42000 [==============================] - 6s 154us/step - loss: 2.3101 - acc: 0.1014\n",
            "Lambda: 0.00000001, LR: 0.0100\tEpoch 1/1\n",
            "42000/42000 [==============================] - 6s 142us/step - loss: 2.3040 - acc: 0.0998\n",
            "Lambda: 0.00000001, LR: 0.0010\tEpoch 1/1\n",
            "42000/42000 [==============================] - 6s 148us/step - loss: 2.2875 - acc: 0.1321\n",
            "Lambda: 0.00000001, LR: 0.0001\tEpoch 1/1\n",
            "42000/42000 [==============================] - 7s 162us/step - loss: 2.3121 - acc: 0.1151\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eUDdy35SDan1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Documentation\n",
        "\n",
        "Set Hyperparameters( High value for lr and low values for lambda)\n",
        "\n",
        "Using lambda values and learning rate values, and running through the loops, we get following observations:\n",
        "\n",
        "* Generally Learning Rate: `1e-3` is a good starting point to compare results. \n",
        "\n",
        "* Increasing Learning Rate increases the loss since it might miss the global minima in the race of finding local minima.\n",
        "\n",
        "* Too small LR generally makes the process slower. (For example for 1e-4 it took 7 seconds, compared to other LRs)\n",
        "\n",
        "### Hyperparameter Optimization\n",
        "\n",
        "### Cross validation Strategy\n",
        "\n",
        "\n",
        "- Do coarse -> fine cross-validation in stages\n",
        "\n",
        "- First stage: only a few epochs to get rough idea of what params work\n",
        "- Second stage: longer running time, finer search\n",
        "-  (repeat as necessary)\n",
        "\n",
        "### Tip for detecting explosions in the solver: \n",
        "- If the cost is ever > 3 * original cost, break out early\n"
      ]
    },
    {
      "metadata": {
        "id": "2K2fWgh75fgm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7785
        },
        "outputId": "c55de328-4756-45b3-f54c-9e151d8e4a52"
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "for k in range(1,20):\n",
        "    lr = math.pow(10, np.random.uniform(-7.0, 4.0))\n",
        "    Lambda = math.pow(3, np.random.uniform(-5,5))\n",
        "    best_acc = train_and_test_loop1(10, lr, Lambda)\n",
        "    print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(k, 100, best_acc[1], lr, Lambda))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 5s 115us/step - loss: 54417.4619 - acc: 0.1007\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 30330.2510 - acc: 0.1005\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 30029.4003 - acc: 0.0987\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 32973.1609 - acc: 0.0986\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 29394.7007 - acc: 0.1003\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 25957.5301 - acc: 0.0992\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 28184.9484 - acc: 0.1005\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 26256.2947 - acc: 0.1010\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 50813.3300 - acc: 0.0997\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 3s 62us/step - loss: 22963.9585 - acc: 0.0995\n",
            "18000/18000 [==============================] - 2s 123us/step\n",
            "Try 1/100: Best_val_acc: 0.09644444444444444, lr: 0.12340030878155088, Lambda: 0.023590440671911263\n",
            "\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 5s 115us/step - loss: 24.8734 - acc: 0.1077\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 3s 62us/step - loss: 19.7726 - acc: 0.1050\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 17.6638 - acc: 0.1027\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 16.2366 - acc: 0.1020\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 3s 62us/step - loss: 15.1586 - acc: 0.1016\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 3s 62us/step - loss: 14.3011 - acc: 0.1017\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 3s 62us/step - loss: 13.6018 - acc: 0.1010\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 13.0160 - acc: 0.1011\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 3s 62us/step - loss: 12.5102 - acc: 0.1024\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 3s 62us/step - loss: 12.0645 - acc: 0.1008\n",
            "18000/18000 [==============================] - 2s 123us/step\n",
            "Try 2/100: Best_val_acc: 0.1015, lr: 3.7672921993450094e-05, Lambda: 0.11112882690967721\n",
            "\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 5s 116us/step - loss: nan - acc: 0.0998\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: nan - acc: 0.0997\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: nan - acc: 0.0997\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: nan - acc: 0.0997\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 3s 62us/step - loss: nan - acc: 0.0997\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 3s 62us/step - loss: nan - acc: 0.0997\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: nan - acc: 0.0997\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 3s 62us/step - loss: nan - acc: 0.0997\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 3s 62us/step - loss: nan - acc: 0.0997\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: nan - acc: 0.0997\n",
            "18000/18000 [==============================] - 2s 125us/step\n",
            "Try 3/100: Best_val_acc: 0.10077777777777777, lr: 459.38972357124874, Lambda: 0.015409197673682961\n",
            "\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 5s 116us/step - loss: nan - acc: 0.0996\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: nan - acc: 0.0997\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: nan - acc: 0.0997\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: nan - acc: 0.0997\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: nan - acc: 0.0997\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: nan - acc: 0.0997\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: nan - acc: 0.0997\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: nan - acc: 0.0997\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: nan - acc: 0.0997\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 3s 62us/step - loss: nan - acc: 0.0997\n",
            "18000/18000 [==============================] - 2s 126us/step\n",
            "Try 4/100: Best_val_acc: 0.10077777777777777, lr: 1.696241659122205, Lambda: 8.164957072802013\n",
            "\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 5s 117us/step - loss: 7.1263 - acc: 0.1025\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 6.5959 - acc: 0.0999\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 6.5208 - acc: 0.0992\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 6.5191 - acc: 0.1009\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 6.4973 - acc: 0.0991\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 3s 62us/step - loss: 6.4963 - acc: 0.0993\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 3s 62us/step - loss: 6.4995 - acc: 0.1005\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 3s 62us/step - loss: 6.4936 - acc: 0.0989\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 3s 62us/step - loss: 6.4883 - acc: 0.0988\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 6.4891 - acc: 0.0995\n",
            "18000/18000 [==============================] - 2s 126us/step\n",
            "Try 5/100: Best_val_acc: 0.10044444444444445, lr: 0.026317409823372457, Lambda: 0.006952274899772121\n",
            "\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 5s 119us/step - loss: nan - acc: 0.0996\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: nan - acc: 0.0997\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: nan - acc: 0.0997\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: nan - acc: 0.0997\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: nan - acc: 0.0997\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: nan - acc: 0.0997\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: nan - acc: 0.0997\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: nan - acc: 0.0997\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: nan - acc: 0.0997\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: nan - acc: 0.0997\n",
            "18000/18000 [==============================] - 2s 129us/step\n",
            "Try 6/100: Best_val_acc: 0.10077777777777777, lr: 222.69234795999753, Lambda: 0.39993764293091205\n",
            "\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 5s 120us/step - loss: 11.3386 - acc: 0.1017\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 8.1792 - acc: 0.1078\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 8.0881 - acc: 0.1080\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 8.0044 - acc: 0.1071\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 7.9249 - acc: 0.1079\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 7.8492 - acc: 0.1074\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 7.7771 - acc: 0.1068\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 7.7087 - acc: 0.1062\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 7.6435 - acc: 0.1059\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 7.5817 - acc: 0.1059\n",
            "18000/18000 [==============================] - 2s 130us/step\n",
            "Try 7/100: Best_val_acc: 0.10116666666666667, lr: 4.1368497645617264e-06, Lambda: 0.02492058050449416\n",
            "\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 5s 121us/step - loss: nan - acc: 0.0995\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: nan - acc: 0.0997\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: nan - acc: 0.0997\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: nan - acc: 0.0997\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: nan - acc: 0.0997\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: nan - acc: 0.0997\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: nan - acc: 0.0997\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: nan - acc: 0.0997\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: nan - acc: 0.0997\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 3s 69us/step - loss: nan - acc: 0.0997\n",
            "18000/18000 [==============================] - 3s 143us/step\n",
            "Try 8/100: Best_val_acc: 0.10077777777777777, lr: 0.08609722407885984, Lambda: 70.56448803837188\n",
            "\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 6s 134us/step - loss: 5118.7960 - acc: 0.0990\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2593.8054 - acc: 0.0995\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 2908.9118 - acc: 0.0986\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 3773.1677 - acc: 0.1004\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2175.6039 - acc: 0.0986\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 2340.6721 - acc: 0.0991\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2630.2663 - acc: 0.0993\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 2509.4872 - acc: 0.1014\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 2308.0034 - acc: 0.0991\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 2255.2251 - acc: 0.0994\n",
            "18000/18000 [==============================] - 2s 132us/step\n",
            "Try 9/100: Best_val_acc: 0.10177777777777777, lr: 0.5685522369932066, Lambda: 0.0048824202243378705\n",
            "\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 5s 123us/step - loss: 2381672.0345 - acc: 0.0984\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 3s 69us/step - loss: 155498.7854 - acc: 0.1029\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 3s 69us/step - loss: 760880005.6719 - acc: 0.0991\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 142499.0343 - acc: 0.1010\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 202772.3161 - acc: 0.0969\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 144038.1127 - acc: 0.0970\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 166222.3057 - acc: 0.0999\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 124223.9021 - acc: 0.0987\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 129342.8848 - acc: 0.1003\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 136097.2882 - acc: 0.1005\n",
            "18000/18000 [==============================] - 2s 137us/step\n",
            "Try 10/100: Best_val_acc: 0.09972222222222223, lr: 0.7884812893424665, Lambda: 0.013400107962307086\n",
            "\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 5s 125us/step - loss: 172.0097 - acc: 0.0969\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 97.6364 - acc: 0.0955\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 80.8438 - acc: 0.1008\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 79.1366 - acc: 0.1029\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 78.5150 - acc: 0.1032\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 77.9205 - acc: 0.1031\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 77.3375 - acc: 0.1028\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 76.7673 - acc: 0.1024\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 76.2064 - acc: 0.1026\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 75.6611 - acc: 0.1027\n",
            "18000/18000 [==============================] - 2s 137us/step\n",
            "Try 11/100: Best_val_acc: 0.10188888888888889, lr: 1.5019292430315417e-07, Lambda: 0.30682473588436865\n",
            "\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 5s 126us/step - loss: 18150737643207.2148 - acc: 0.1001\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 64417398.8286 - acc: 0.1011\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 33892588.4450 - acc: 0.1013\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 37587041.9718 - acc: 0.1007\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 45102461.3086 - acc: 0.0991\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 41054802.2796 - acc: 0.0995\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 62723272.7680 - acc: 0.0977\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 73242612.2023 - acc: 0.0997\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 47440938.9630 - acc: 0.1007\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 3s 63us/step - loss: 43452797.7798 - acc: 0.0985\n",
            "18000/18000 [==============================] - 2s 138us/step\n",
            "Try 12/100: Best_val_acc: 0.10188888888888889, lr: 1.7334757787277646, Lambda: 0.05244638851050594\n",
            "\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 5s 128us/step - loss: nan - acc: 0.0997\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: nan - acc: 0.0997\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: nan - acc: 0.0997\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: nan - acc: 0.0997\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: nan - acc: 0.0997\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: nan - acc: 0.0997\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: nan - acc: 0.0997\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: nan - acc: 0.0997\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: nan - acc: 0.0997\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: nan - acc: 0.0997\n",
            "18000/18000 [==============================] - 3s 140us/step\n",
            "Try 13/100: Best_val_acc: 0.10077777777777777, lr: 3575.124867397451, Lambda: 60.11094595324173\n",
            "\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 5s 128us/step - loss: nan - acc: 0.0996\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: nan - acc: 0.0997\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: nan - acc: 0.0997\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: nan - acc: 0.0997\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: nan - acc: 0.0997\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: nan - acc: 0.0997\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: nan - acc: 0.0997\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: nan - acc: 0.0997\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: nan - acc: 0.0997\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: nan - acc: 0.0997\n",
            "18000/18000 [==============================] - 3s 141us/step\n",
            "Try 14/100: Best_val_acc: 0.10077777777777777, lr: 0.6642020564252009, Lambda: 158.46692767397266\n",
            "\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 5s 131us/step - loss: 47693861.7667 - acc: 0.1011\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 50668157.9269 - acc: 0.0999\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 57786065.0781 - acc: 0.0983\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 27887881.3441 - acc: 0.0993\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 11221923.8057 - acc: 0.1013\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 39785953.5478 - acc: 0.1001\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 50824171.7970 - acc: 0.1018\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 47169471.1276 - acc: 0.1006\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 54001573.4884 - acc: 0.0997\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 38669640.0971 - acc: 0.1004\n",
            "18000/18000 [==============================] - 3s 141us/step\n",
            "Try 15/100: Best_val_acc: 0.09905555555555555, lr: 7.02684474949737e-06, Lambda: 214.29924676855768\n",
            "\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 6s 133us/step - loss: 17476951.5845 - acc: 0.0987\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 4044572.0384 - acc: 0.0991\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 1842444.1809 - acc: 0.1002\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2528885.3620 - acc: 0.0999\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2457504.3907 - acc: 0.0999\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 1567640.9868 - acc: 0.1014\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 1687686.9246 - acc: 0.0999\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2015363.2244 - acc: 0.1009\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2593221.6975 - acc: 0.1003\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 2196077.1608 - acc: 0.0992\n",
            "18000/18000 [==============================] - 3s 141us/step\n",
            "Try 16/100: Best_val_acc: 0.09844444444444445, lr: 0.0029731267416630057, Lambda: 1.2264544677873963\n",
            "\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 6s 132us/step - loss: 5148.2287 - acc: 0.1015\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 4768.3065 - acc: 0.0978\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 4725.9113 - acc: 0.1008\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 4724.9312 - acc: 0.1030\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 4715.5616 - acc: 0.1004\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 4721.2781 - acc: 0.1009\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 4717.6315 - acc: 0.1014\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 4713.1741 - acc: 0.0984\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 4714.8051 - acc: 0.1030\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: 4715.6165 - acc: 0.1016\n",
            "18000/18000 [==============================] - 3s 145us/step\n",
            "Try 17/100: Best_val_acc: 0.09522222222222222, lr: 4.603405916216989e-05, Lambda: 5.566096325649072\n",
            "\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 6s 134us/step - loss: 11333.3030 - acc: 0.0994\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 9418.0646 - acc: 0.1016\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 8286.7084 - acc: 0.1010\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 3s 66us/step - loss: 7745.8007 - acc: 0.1006\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 3s 73us/step - loss: 7464.0518 - acc: 0.0988\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 3s 73us/step - loss: 7306.2983 - acc: 0.0986\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 3s 73us/step - loss: 7156.4961 - acc: 0.0989\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 3s 66us/step - loss: 7087.0619 - acc: 0.0988\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 7019.5388 - acc: 0.0997\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: 6980.3618 - acc: 0.0967\n",
            "18000/18000 [==============================] - 3s 146us/step\n",
            "Try 18/100: Best_val_acc: 0.0945, lr: 1.5318302137500956e-06, Lambda: 37.08134687279594\n",
            "\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 6s 135us/step - loss: nan - acc: 0.0998\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 3s 64us/step - loss: nan - acc: 0.0997\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: nan - acc: 0.0997\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: nan - acc: 0.0997\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: nan - acc: 0.0997\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 3s 69us/step - loss: nan - acc: 0.0997\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 3s 68us/step - loss: nan - acc: 0.0997\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: nan - acc: 0.0997\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: nan - acc: 0.0997\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 3s 65us/step - loss: nan - acc: 0.0997\n",
            "18000/18000 [==============================] - 3s 147us/step\n",
            "Try 19/100: Best_val_acc: 0.10077777777777777, lr: 1619.8690452011406, Lambda: 0.29738580545013826\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AwGzLMozPgrw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Now run finer search"
      ]
    },
    {
      "metadata": {
        "id": "CRbkGDgOGydf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "outputId": "8595ecae-bcf9-4e85-8bb4-576a1ea14ce1"
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "for k in range(1,10):\n",
        "    lr = math.pow(10, np.random.uniform(-3.0, -2.0))\n",
        "    Lambda = math.pow(10, np.random.uniform(-5,2))\n",
        "    best_acc = train_and_test_loop(1, lr, Lambda, False)\n",
        "    print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(k, 100, best_acc, lr, Lambda))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "42000/42000 [==============================] - 6s 140us/step - loss: 2.6885 - acc: 0.1115\n",
            "Try 1/100: Best_val_acc: None, lr: 0.0020040290996236726, Lambda: 0.003061818286394544\n",
            "\n",
            "Epoch 1/1\n",
            "42000/42000 [==============================] - 6s 139us/step - loss: nan - acc: 0.0992\n",
            "Try 2/100: Best_val_acc: None, lr: 0.0020966739380652967, Lambda: 15.410392818767882\n",
            "\n",
            "Epoch 1/1\n",
            "42000/42000 [==============================] - 6s 140us/step - loss: nan - acc: 0.1008\n",
            "Try 3/100: Best_val_acc: None, lr: 0.006708929668292378, Lambda: 1.5361792252962088\n",
            "\n",
            "Epoch 1/1\n",
            "42000/42000 [==============================] - 6s 142us/step - loss: nan - acc: 0.0985\n",
            "Try 4/100: Best_val_acc: None, lr: 0.003006232764717569, Lambda: 4.769587324404851\n",
            "\n",
            "Epoch 1/1\n",
            "42000/42000 [==============================] - 6s 143us/step - loss: nan - acc: 0.0994\n",
            "Try 5/100: Best_val_acc: None, lr: 0.005565223875129477, Lambda: 20.414903250247587\n",
            "\n",
            "Epoch 1/1\n",
            "42000/42000 [==============================] - 6s 145us/step - loss: 5.1812 - acc: 0.1004\n",
            "Try 6/100: Best_val_acc: None, lr: 0.0016601208620742106, Lambda: 0.015282305105081415\n",
            "\n",
            "Epoch 1/1\n",
            "42000/42000 [==============================] - 6s 153us/step - loss: 526.8013 - acc: 0.1008\n",
            "Try 7/100: Best_val_acc: None, lr: 0.0013429704599653932, Lambda: 0.1971097913064156\n",
            "\n",
            "Epoch 1/1\n",
            "42000/42000 [==============================] - 7s 158us/step - loss: 9.5147 - acc: 0.1004\n",
            "Try 8/100: Best_val_acc: None, lr: 0.001350411540914494, Lambda: 0.029678225654200373\n",
            "\n",
            "Epoch 1/1\n",
            "42000/42000 [==============================] - 6s 149us/step - loss: 3.6347 - acc: 0.1032\n",
            "Try 9/100: Best_val_acc: None, lr: 0.0014170340625411333, Lambda: 0.009400098754779086\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}